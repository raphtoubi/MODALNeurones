{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import os as os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLabel(label): #gives a nb to the aminoacid\n",
    "    aminoacidList = [\"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\",\n",
    "                    \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"PYL\", \"SEL\", \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"]\n",
    "    for i in range (0, 22):\n",
    "        if label == aminoacidList[i]:\n",
    "            return i\n",
    "    print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "\n",
    "\n",
    "    labelList = []\n",
    "\n",
    "    with open(filename) as f:\n",
    "\n",
    "        nbOfSamples = int(f.readline())\n",
    "        i = 0\n",
    "\n",
    "        shapes = f.readline() # length of sequence + nb of features\n",
    "        input_shape = int(shapes.split(\",\")[1])\n",
    "\n",
    "        # inputs\n",
    "        distancesList = np.zeros((nbOfSamples, input_shape))\n",
    "        previousResiduesList = np.zeros((nbOfSamples, 7))\n",
    "        \n",
    "        # outputs\n",
    "        labelList = np.zeros(nbOfSamples)\n",
    "        \n",
    "\n",
    "\n",
    "        while i < nbOfSamples:\n",
    "            # Extracting labels\n",
    "            sequence = f.readline().split(\",\")\n",
    "            labelList[i] = convertLabel(sequence[8])\n",
    "            previousResiduesList[i, :] = np.array([convertLabel(sequence[i]) for i in range(0, 7)])\n",
    "\n",
    "            # Extracting distances\n",
    "            distancesString = f.readline()\n",
    "            distancesArray = np.array([float(s) for s in distancesString.split(\",\")])\n",
    "            distancesList[i, :] = distancesArray\n",
    "\n",
    "            # empty line\n",
    "            f.readline()\n",
    "\n",
    "            i += 1\n",
    "    return input_shape, distancesList, previousResiduesList, labelList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def single_input_model():\n",
    "    # activation function: leaky ReLU\n",
    "    leakyrelu = lambda x: tf.keras.activations.relu(x, alpha=0.01, max_value=None, threshold=0)\n",
    "\n",
    "    # creation du reseau de neurones\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        # hidden layer\n",
    "        tf.keras.layers.Dense(units = 105, activation = leakyrelu, input_shape = (105,)),\n",
    "      #  keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 210, activation = leakyrelu),\n",
    "       # keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 210, activation = leakyrelu),\n",
    "        #keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 210, activation = leakyrelu),\n",
    "        tf.keras.layers.Dense(units = 100, activation = leakyrelu),\n",
    "        #keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 75, activation = leakyrelu),\n",
    "        #keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 53, activation = leakyrelu),\n",
    "        #tf.keras.layers.Dropout(rate=0.2),\n",
    "\n",
    "        # final layer\n",
    "        tf.keras.layers.Dense(units = 22, activation = tf.nn.softmax),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkppoints\n",
    "single_input_checkpoint_path = \"Single input model/checkpoints/checkpoint.ckpt\"\n",
    "single_input_checkpoint_dir = os.path.dirname(single_input_checkpoint_path)\n",
    "\n",
    "single_input_cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    single_input_checkpoint_path, save_weights_only=True, verbose=1,\n",
    "    # save weights, every 10-epoch\n",
    "    period=10)\n",
    "\n",
    "single_input_latest = tf.train.latest_checkpoint(single_input_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_input_model():\n",
    "    # activation function: leaky ReLU\n",
    "    leakyrelu = lambda x: tf.keras.activations.relu(x, alpha=0.01, max_value=None, threshold=0)\n",
    "\n",
    "    # distances branch\n",
    "    distancesBranch = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units = 105, activation = leakyrelu, input_shape = (105,)),\n",
    "        #tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 210, activation = leakyrelu),\n",
    "        #tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 210, activation = leakyrelu),\n",
    "        #tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 100, activation = leakyrelu),\n",
    "       # tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 75, activation = leakyrelu),\n",
    "        #tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units = 53, activation = leakyrelu),\n",
    "        #tf.keras.layers.Dropout(rate=0.2)\n",
    "    ])\n",
    "    \n",
    "    # residues branch   \n",
    "    residuesBranch = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units = 7, activation = leakyrelu, input_shape = (7,))\n",
    "    ])\n",
    "    \n",
    "    # concatenation\n",
    "    combinedInput = tf.keras.layers.concatenate([distancesBranch.output, residuesBranch.output])\n",
    "\n",
    "    x = tf.keras.layers.Dense(units = 30, activation = leakyrelu, input_shape = (59,))(combinedInput)\n",
    "    x = tf.keras.layers.Dense(units = 22, activation = tf.nn.softmax)(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = [distancesBranch.input, residuesBranch.input], outputs = x)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkppoints\n",
    "double_input_checkpoint_path = \"Double input model/checkpoints/checkpoint.ckpt\"\n",
    "double_input_checkpoint_dir = os.path.dirname(double_input_checkpoint_path)\n",
    "\n",
    "double_input_cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    double_input_checkpoint_path, save_weights_only=True, verbose=1,\n",
    "    # save weights, every 10-epoch\n",
    "    period=10)\n",
    "\n",
    "double_input_latest = tf.train.latest_checkpoint(double_input_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = ''\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "input_shape, distances, previousResidues, output = load_data(\"Data/matrices.csv\")\n",
    "\n",
    "\n",
    "distances_train, distances_test, residues_train, residues_test, y_train, y_test = train_test_split(\n",
    "    distances, previousResidues, output, test_size=0.2, shuffle = False, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the single input model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "single_input_sess = tf.Session()\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "single_input_model = single_input_model()\n",
    "\n",
    "if single_input_latest: single_input_model.load_weights(single_input_latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 976 samples, validate on 245 samples\n",
      "Epoch 1/20\n",
      "976/976 [==============================] - 1s 665us/sample - loss: 3.0855 - acc: 0.0840 - val_loss: 2.9130 - val_acc: 0.0980\n",
      "Epoch 2/20\n",
      "976/976 [==============================] - 0s 194us/sample - loss: 2.9139 - acc: 0.0984 - val_loss: 2.8956 - val_acc: 0.1143\n",
      "Epoch 3/20\n",
      "976/976 [==============================] - 0s 199us/sample - loss: 2.8903 - acc: 0.1117 - val_loss: 2.8457 - val_acc: 0.0857\n",
      "Epoch 4/20\n",
      "976/976 [==============================] - 0s 192us/sample - loss: 2.8318 - acc: 0.1168 - val_loss: 2.8222 - val_acc: 0.1224\n",
      "Epoch 5/20\n",
      "976/976 [==============================] - 0s 194us/sample - loss: 2.8006 - acc: 0.1168 - val_loss: 2.7707 - val_acc: 0.1143\n",
      "Epoch 6/20\n",
      "976/976 [==============================] - 0s 196us/sample - loss: 2.7872 - acc: 0.1322 - val_loss: 2.8044 - val_acc: 0.0939\n",
      "Epoch 7/20\n",
      "976/976 [==============================] - 0s 200us/sample - loss: 2.7843 - acc: 0.1260 - val_loss: 2.7683 - val_acc: 0.1347\n",
      "Epoch 8/20\n",
      "976/976 [==============================] - 0s 194us/sample - loss: 2.7261 - acc: 0.1496 - val_loss: 2.7497 - val_acc: 0.1388\n",
      "Epoch 9/20\n",
      "976/976 [==============================] - 0s 207us/sample - loss: 2.7488 - acc: 0.1373 - val_loss: 2.7514 - val_acc: 0.1388\n",
      "Epoch 10/20\n",
      "768/976 [======================>.......] - ETA: 0s - loss: 2.7077 - acc: 0.1641\n",
      "Epoch 00010: saving model to Single input model/checkpoints/checkpoint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x128a1ca90>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x128a1ca90>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976/976 [==============================] - 1s 1ms/sample - loss: 2.7168 - acc: 0.1547 - val_loss: 2.7337 - val_acc: 0.1265\n",
      "Epoch 11/20\n",
      "976/976 [==============================] - 0s 186us/sample - loss: 2.6971 - acc: 0.1455 - val_loss: 2.6666 - val_acc: 0.1429\n",
      "Epoch 12/20\n",
      "976/976 [==============================] - 0s 203us/sample - loss: 2.6285 - acc: 0.1732 - val_loss: 2.7084 - val_acc: 0.1510\n",
      "Epoch 13/20\n",
      "976/976 [==============================] - 0s 203us/sample - loss: 2.5860 - acc: 0.1752 - val_loss: 2.6287 - val_acc: 0.1673\n",
      "Epoch 14/20\n",
      "976/976 [==============================] - 0s 207us/sample - loss: 2.6110 - acc: 0.1814 - val_loss: 2.6508 - val_acc: 0.1551\n",
      "Epoch 15/20\n",
      "976/976 [==============================] - 0s 210us/sample - loss: 2.5639 - acc: 0.1855 - val_loss: 2.5885 - val_acc: 0.1878\n",
      "Epoch 16/20\n",
      "976/976 [==============================] - 0s 202us/sample - loss: 2.5211 - acc: 0.2080 - val_loss: 2.6334 - val_acc: 0.1510\n",
      "Epoch 17/20\n",
      "976/976 [==============================] - 0s 194us/sample - loss: 2.4907 - acc: 0.2213 - val_loss: 2.5163 - val_acc: 0.1469\n",
      "Epoch 18/20\n",
      "976/976 [==============================] - 0s 193us/sample - loss: 2.4384 - acc: 0.2305 - val_loss: 2.5508 - val_acc: 0.1673\n",
      "Epoch 19/20\n",
      "976/976 [==============================] - 0s 193us/sample - loss: 2.3863 - acc: 0.2285 - val_loss: 2.5121 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "672/976 [===================>..........] - ETA: 0s - loss: 2.3436 - acc: 0.2426\n",
      "Epoch 00020: saving model to Single input model/checkpoints/checkpoint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x128a1ca90>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x128a1ca90>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "976/976 [==============================] - 1s 834us/sample - loss: 2.3321 - acc: 0.2449 - val_loss: 2.5877 - val_acc: 0.1633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12a8b2be0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_input_model.fit(distances_train, y_train,\n",
    "                       validation_data = (distances_test, y_test),\n",
    "                       callbacks = [single_input_cp_callback], epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bcc56d3fdc1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msingle_input_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Single input model/single_input_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "single_input_model.save(\"Single input model/single_input_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the double input model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "double_input_model = double_input_model()\n",
    "\n",
    "if double_input_latest: double_input_model.load_weights(double_input_latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 976 samples, validate on 245 samples\n",
      "Epoch 1/20\n",
      "976/976 [==============================] - 1s 1ms/sample - loss: 4.4298 - acc: 0.0441 - val_loss: 3.2175 - val_acc: 0.0776\n",
      "Epoch 2/20\n",
      "976/976 [==============================] - 0s 181us/sample - loss: 3.1670 - acc: 0.0758 - val_loss: 3.0022 - val_acc: 0.1102\n",
      "Epoch 3/20\n",
      "976/976 [==============================] - 0s 180us/sample - loss: 2.9788 - acc: 0.1066 - val_loss: 2.9148 - val_acc: 0.0857\n",
      "Epoch 4/20\n",
      "976/976 [==============================] - 0s 181us/sample - loss: 2.9207 - acc: 0.1055 - val_loss: 2.9224 - val_acc: 0.1265\n",
      "Epoch 5/20\n",
      "976/976 [==============================] - 0s 236us/sample - loss: 2.8767 - acc: 0.1260 - val_loss: 2.8458 - val_acc: 0.0980\n",
      "Epoch 6/20\n",
      "976/976 [==============================] - 0s 242us/sample - loss: 2.8371 - acc: 0.1240 - val_loss: 2.8114 - val_acc: 0.1306\n",
      "Epoch 7/20\n",
      "976/976 [==============================] - 0s 227us/sample - loss: 2.7985 - acc: 0.1393 - val_loss: 2.7845 - val_acc: 0.1184\n",
      "Epoch 8/20\n",
      "976/976 [==============================] - 0s 202us/sample - loss: 2.7812 - acc: 0.1322 - val_loss: 2.7566 - val_acc: 0.1551\n",
      "Epoch 9/20\n",
      "976/976 [==============================] - 0s 182us/sample - loss: 2.7510 - acc: 0.1588 - val_loss: 2.7353 - val_acc: 0.1633\n",
      "Epoch 10/20\n",
      "672/976 [===================>..........] - ETA: 0s - loss: 2.7151 - acc: 0.1741\n",
      "Epoch 00010: saving model to Double input model/checkpoints/checkpoint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x12b154e10>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x12b154e10>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976/976 [==============================] - 0s 435us/sample - loss: 2.7155 - acc: 0.1701 - val_loss: 2.7514 - val_acc: 0.1510\n",
      "Epoch 11/20\n",
      "976/976 [==============================] - 0s 236us/sample - loss: 2.7121 - acc: 0.1629 - val_loss: 2.6847 - val_acc: 0.1918\n",
      "Epoch 12/20\n",
      "976/976 [==============================] - 0s 177us/sample - loss: 2.6691 - acc: 0.1875 - val_loss: 2.6593 - val_acc: 0.1918\n",
      "Epoch 13/20\n",
      "976/976 [==============================] - 0s 182us/sample - loss: 2.6635 - acc: 0.1793 - val_loss: 2.6517 - val_acc: 0.2082\n",
      "Epoch 14/20\n",
      "976/976 [==============================] - 0s 193us/sample - loss: 2.6588 - acc: 0.1855 - val_loss: 2.6542 - val_acc: 0.1796\n",
      "Epoch 15/20\n",
      "976/976 [==============================] - 0s 232us/sample - loss: 2.6213 - acc: 0.1895 - val_loss: 2.6139 - val_acc: 0.1755\n",
      "Epoch 16/20\n",
      "976/976 [==============================] - 0s 190us/sample - loss: 2.5837 - acc: 0.1936 - val_loss: 2.5941 - val_acc: 0.2041\n",
      "Epoch 17/20\n",
      "976/976 [==============================] - 0s 218us/sample - loss: 2.5361 - acc: 0.2121 - val_loss: 2.5324 - val_acc: 0.2367\n",
      "Epoch 18/20\n",
      "976/976 [==============================] - 0s 221us/sample - loss: 2.5450 - acc: 0.2162 - val_loss: 2.5544 - val_acc: 0.2041\n",
      "Epoch 19/20\n",
      "976/976 [==============================] - 0s 207us/sample - loss: 2.5247 - acc: 0.2172 - val_loss: 2.7549 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "640/976 [==================>...........] - ETA: 0s - loss: 2.5539 - acc: 0.2125\n",
      "Epoch 00020: saving model to Double input model/checkpoints/checkpoint.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x12b154e10>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x12b154e10>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "976/976 [==============================] - 1s 729us/sample - loss: 2.5411 - acc: 0.2152 - val_loss: 2.5441 - val_acc: 0.1918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12c3c5400>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_input_model.fit([distances_train, residues_train], y_train,\n",
    "                       validation_data = ([distances_test, residues_test], y_test),\n",
    "                       callbacks = [double_input_cp_callback], epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph = freeze_session(tf.keras.backend.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, './', \"Double input model/double_input_model.pb\", as_text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
